defaults:
  - _self_
  - task: lift_image_abs
  # - task: can_image_abs
  # - task: square_image_abs

name: train_latent_diffusion
_target_: diffusion_policy.workspace.train_latent_diffusion_workspace.TrainLatentDiffusionWorkspace

task_name: ${task.name}
shape_meta: ${task.shape_meta}
exp_name: "default"

resume_ckpt_path: /home/zzy/robot/robot_zzy/diffusion_policy/data/outputs/2024.08.12/23.33.05_train_latent_diffusion_robomimic_mixed_image/last.ckpt
# ae_path: data/latent_diffusion_policy_ae/2024-08-05_20-33-28/last.ckpt # lift
# ae_workspace_path: data/outputs/2024.08.11/20.12.11_train_ae_can_lowdim/last.ckpt
ae_workspace_path: data/outputs/2024.08.12/19.23.57_train_ae_robomimic_mixed_lowdim/last.ckpt
# ae_path: data/latent_diffusion_policy_ae/2024-08-06_23-18-53/last.ckpt # square

seed: 3407

horizon: 16
n_obs_steps: 1
n_action_steps: 16
n_latency_steps: 0
dataset_obs_steps: ${n_obs_steps}
past_action_visible: False
keypoint_visible_rate: 1.0
obs_as_global_cond: True
load_image_features: False

policy:
  _target_: diffusion_policy.policy.latent_diffusion_policy.LatentDiffusionPolicy

  image_feature_dim: 512
  load_image_features: ${load_image_features}
  shape_meta: ${shape_meta}
  # noise_scheduler:
    # _target_: diffusers.schedulers.scheduling_ddpm.DDPMScheduler
  num_train_timesteps: 100
  beta_start: 0.0001
  beta_end: 0.02
  num_inference_timesteps: 100
  beta_schedule: squaredcos_cap_v2
  variance_type: fixed_small # Yilun's paper uses fixed_small_log instead, but easy to cause Nan
  clip_sample: True # required when predict_epsilon=False
  prediction_type: epsilon # or sample
  # denoiser
  n_layers: 8
  n_heads: 8
  hidden_size: 1024
  horizon: ${horizon}
  dropout: 0.1
  lr: 2e-4
  use_lr_scheduler: True
  warmup_steps: 1000
  task_name: ${task.name}
  # n_action_steps: ${eval:'${n_action_steps}+${n_latency_steps}'}
  # n_obs_steps: ${n_obs_steps}
  # num_inference_steps: 100
  # obs_as_global_cond: ${obs_as_global_cond}
  # crop_shape: [76, 76]
  # crop_shape: null
  # diffusion_step_embed_dim: 128
  # down_dims: [512, 1024, 2048]
  # kernel_size: 5
  # n_groups: 8
  # cond_predict_scale: True
  # obs_encoder_group_norm: True
  # eval_fixed_crop: True

  # scheduler.step params
  # predict_epsilon: True

# ema:
#   _target_: diffusion_policy.model.diffusion.ema_model.EMAModel
#   update_after_step: 0
#   inv_gamma: 1.0
#   power: 0.75
#   min_value: 0.0
#   max_value: 0.9999


datamodule:
  _target_: diffusion_policy.dataset.robomimic_replay_image_dataset.RobomimicImageDatamodule
  batch_size: 2048
  num_workers: 32
  dataset: ${task.dataset}

# optimizer:
#   _target_: torch.optim.AdamW
#   lr: 1.0e-4
#   betas: [0.95, 0.999]
#   eps: 1.0e-8
#   weight_decay: 1.0e-6

trainer:
  _target_: lightning.pytorch.trainer.Trainer
  max_epochs: -1
  max_steps: 10000
  precision: bf16-mixed
  check_val_every_n_epoch: 
  val_check_interval: 500
  log_every_n_steps: 1
  strategy: ddp_find_unused_parameters_true
  logger:
    _target_: lightning.pytorch.loggers.wandb.WandbLogger 
    project: latent_diffusion_policy_ldm
    name: ${now:%Y.%m.%d-%H.%M.%S}_${name}_${task_name}
    save_dir: data/outputs/${now:%Y.%m.%d}/${now:%H.%M.%S}_${name}_${task_name}
  num_sanity_val_steps: 2
  # overfit_batches: 1
  fast_dev_run: False
  # profiler: simple
  # profiler: 
  #   _target_: lightning.pytorch.profilers.AdvancedProfiler
  #   dirpath: .
  #   filename: perf_logs
  gradient_clip_val: 1
  callbacks:
    - _target_: diffusion_policy.workspace.workspace_checkpoint_callback.WorkspaceCheckpointCallback
      save_top_k: 1
      save_last: True
      save_weights_only: False
      monitor: val/denoise_loss
      mode: min
      dirpath: data/outputs/${now:%Y.%m.%d}/${now:%H.%M.%S}_${name}_${task_name}
    - _target_: lightning.pytorch.callbacks.TQDMProgressBar
    - _target_: diffusion_policy.workspace.env_runner_test_callback.EnvRunnerTestCallback
    # - _target_: diffusion_policy.common.ema_callback.EMACallback
      # decay: 0.9999

training:
  use_ema: False
#   device: "cuda:0"
#   seed: 42
#   debug: False
#   resume: False
#   # optimization
#   lr_scheduler: cosine
#   lr_warmup_steps: 500
#   num_epochs: 3050
#   gradient_accumulate_every: 1
#   # EMA destroys performance when used with BatchNorm
#   # replace BatchNorm with GroupNorm.
#   # training loop control
#   # in epochs
#   rollout_every: 50
#   checkpoint_every: 50
#   val_every: 1
#   sample_every: 5
#   # steps per epoch
#   max_train_steps: null
#   max_val_steps: null
#   # misc
#   tqdm_interval_sec: 1.0

logging:
  project: diffusion_policy_debug
  resume: True
  mode: online
  name: ${now:%Y.%m.%d-%H.%M.%S}_${name}_${task_name}
  tags: ["${name}", "${task_name}", "${exp_name}"]
  id: null
  group: null

checkpoint:
  topk:
    monitor_key: test_mean_score
    mode: max
    k: 5
    format_str: 'epoch={epoch:04d}-test_mean_score={test_mean_score:.3f}.ckpt'
  save_last_ckpt: True
  save_last_snapshot: False

multi_run:
  run_dir: data/outputs/${now:%Y.%m.%d}/${now:%H.%M.%S}_${name}_${task_name}
  wandb_name_base: ${now:%Y.%m.%d-%H.%M.%S}_${name}_${task_name}

hydra:
  job:
    override_dirname: ${name}
  run:
    dir: data/outputs/${now:%Y.%m.%d}/${now:%H.%M.%S}_${name}_${task_name}
  sweep:
    dir: data/outputs/${now:%Y.%m.%d}/${now:%H.%M.%S}_${name}_${task_name}
    subdir: ${hydra.job.num}
