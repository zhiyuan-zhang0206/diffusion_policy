{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['_dummy_variable', 'pl_model.pe', 'pl_model.autoencoder.cls', 'pl_model.autoencoder.pe', 'pl_model.autoencoder.action_emb.weight', 'pl_model.autoencoder.action_emb.bias', 'pl_model.autoencoder.z_encoder.encoder.layers.0.self_attn.in_proj_weight', 'pl_model.autoencoder.z_encoder.encoder.layers.0.self_attn.in_proj_bias', 'pl_model.autoencoder.z_encoder.encoder.layers.0.self_attn.out_proj.weight', 'pl_model.autoencoder.z_encoder.encoder.layers.0.self_attn.out_proj.bias', 'pl_model.autoencoder.z_encoder.encoder.layers.0.linear1.weight', 'pl_model.autoencoder.z_encoder.encoder.layers.0.linear1.bias', 'pl_model.autoencoder.z_encoder.encoder.layers.0.linear2.weight', 'pl_model.autoencoder.z_encoder.encoder.layers.0.linear2.bias', 'pl_model.autoencoder.z_encoder.encoder.layers.0.norm1.weight', 'pl_model.autoencoder.z_encoder.encoder.layers.0.norm1.bias', 'pl_model.autoencoder.z_encoder.encoder.layers.0.norm2.weight', 'pl_model.autoencoder.z_encoder.encoder.layers.0.norm2.bias', 'pl_model.autoencoder.z_encoder.encoder.layers.1.self_attn.in_proj_weight', 'pl_model.autoencoder.z_encoder.encoder.layers.1.self_attn.in_proj_bias', 'pl_model.autoencoder.z_encoder.encoder.layers.1.self_attn.out_proj.weight', 'pl_model.autoencoder.z_encoder.encoder.layers.1.self_attn.out_proj.bias', 'pl_model.autoencoder.z_encoder.encoder.layers.1.linear1.weight', 'pl_model.autoencoder.z_encoder.encoder.layers.1.linear1.bias', 'pl_model.autoencoder.z_encoder.encoder.layers.1.linear2.weight', 'pl_model.autoencoder.z_encoder.encoder.layers.1.linear2.bias', 'pl_model.autoencoder.z_encoder.encoder.layers.1.norm1.weight', 'pl_model.autoencoder.z_encoder.encoder.layers.1.norm1.bias', 'pl_model.autoencoder.z_encoder.encoder.layers.1.norm2.weight', 'pl_model.autoencoder.z_encoder.encoder.layers.1.norm2.bias', 'pl_model.autoencoder.z_encoder.encoder.layers.2.self_attn.in_proj_weight', 'pl_model.autoencoder.z_encoder.encoder.layers.2.self_attn.in_proj_bias', 'pl_model.autoencoder.z_encoder.encoder.layers.2.self_attn.out_proj.weight', 'pl_model.autoencoder.z_encoder.encoder.layers.2.self_attn.out_proj.bias', 'pl_model.autoencoder.z_encoder.encoder.layers.2.linear1.weight', 'pl_model.autoencoder.z_encoder.encoder.layers.2.linear1.bias', 'pl_model.autoencoder.z_encoder.encoder.layers.2.linear2.weight', 'pl_model.autoencoder.z_encoder.encoder.layers.2.linear2.bias', 'pl_model.autoencoder.z_encoder.encoder.layers.2.norm1.weight', 'pl_model.autoencoder.z_encoder.encoder.layers.2.norm1.bias', 'pl_model.autoencoder.z_encoder.encoder.layers.2.norm2.weight', 'pl_model.autoencoder.z_encoder.encoder.layers.2.norm2.bias', 'pl_model.autoencoder.z_encoder.ln.weight', 'pl_model.autoencoder.z_encoder.ln.bias', 'pl_model.autoencoder.z_down.weight', 'pl_model.autoencoder.z_down.bias', 'pl_model.autoencoder.z_up.weight', 'pl_model.autoencoder.z_up.bias', 'pl_model.autoencoder.conditioner.encoder.layers.0.self_attn.in_proj_weight', 'pl_model.autoencoder.conditioner.encoder.layers.0.self_attn.in_proj_bias', 'pl_model.autoencoder.conditioner.encoder.layers.0.self_attn.out_proj.weight', 'pl_model.autoencoder.conditioner.encoder.layers.0.self_attn.out_proj.bias', 'pl_model.autoencoder.conditioner.encoder.layers.0.linear1.weight', 'pl_model.autoencoder.conditioner.encoder.layers.0.linear1.bias', 'pl_model.autoencoder.conditioner.encoder.layers.0.linear2.weight', 'pl_model.autoencoder.conditioner.encoder.layers.0.linear2.bias', 'pl_model.autoencoder.conditioner.encoder.layers.0.norm1.weight', 'pl_model.autoencoder.conditioner.encoder.layers.0.norm1.bias', 'pl_model.autoencoder.conditioner.encoder.layers.0.norm2.weight', 'pl_model.autoencoder.conditioner.encoder.layers.0.norm2.bias', 'pl_model.autoencoder.conditioner.encoder.layers.1.self_attn.in_proj_weight', 'pl_model.autoencoder.conditioner.encoder.layers.1.self_attn.in_proj_bias', 'pl_model.autoencoder.conditioner.encoder.layers.1.self_attn.out_proj.weight', 'pl_model.autoencoder.conditioner.encoder.layers.1.self_attn.out_proj.bias', 'pl_model.autoencoder.conditioner.encoder.layers.1.linear1.weight', 'pl_model.autoencoder.conditioner.encoder.layers.1.linear1.bias', 'pl_model.autoencoder.conditioner.encoder.layers.1.linear2.weight', 'pl_model.autoencoder.conditioner.encoder.layers.1.linear2.bias', 'pl_model.autoencoder.conditioner.encoder.layers.1.norm1.weight', 'pl_model.autoencoder.conditioner.encoder.layers.1.norm1.bias', 'pl_model.autoencoder.conditioner.encoder.layers.1.norm2.weight', 'pl_model.autoencoder.conditioner.encoder.layers.1.norm2.bias', 'pl_model.autoencoder.conditioner.encoder.layers.2.self_attn.in_proj_weight', 'pl_model.autoencoder.conditioner.encoder.layers.2.self_attn.in_proj_bias', 'pl_model.autoencoder.conditioner.encoder.layers.2.self_attn.out_proj.weight', 'pl_model.autoencoder.conditioner.encoder.layers.2.self_attn.out_proj.bias', 'pl_model.autoencoder.conditioner.encoder.layers.2.linear1.weight', 'pl_model.autoencoder.conditioner.encoder.layers.2.linear1.bias', 'pl_model.autoencoder.conditioner.encoder.layers.2.linear2.weight', 'pl_model.autoencoder.conditioner.encoder.layers.2.linear2.bias', 'pl_model.autoencoder.conditioner.encoder.layers.2.norm1.weight', 'pl_model.autoencoder.conditioner.encoder.layers.2.norm1.bias', 'pl_model.autoencoder.conditioner.encoder.layers.2.norm2.weight', 'pl_model.autoencoder.conditioner.encoder.layers.2.norm2.bias', 'pl_model.autoencoder.conditioner.ln.weight', 'pl_model.autoencoder.conditioner.ln.bias', 'pl_model.autoencoder.decoder.decoder.layers.0.self_attn.in_proj_weight', 'pl_model.autoencoder.decoder.decoder.layers.0.self_attn.in_proj_bias', 'pl_model.autoencoder.decoder.decoder.layers.0.self_attn.out_proj.weight', 'pl_model.autoencoder.decoder.decoder.layers.0.self_attn.out_proj.bias', 'pl_model.autoencoder.decoder.decoder.layers.0.multihead_attn.in_proj_weight', 'pl_model.autoencoder.decoder.decoder.layers.0.multihead_attn.in_proj_bias', 'pl_model.autoencoder.decoder.decoder.layers.0.multihead_attn.out_proj.weight', 'pl_model.autoencoder.decoder.decoder.layers.0.multihead_attn.out_proj.bias', 'pl_model.autoencoder.decoder.decoder.layers.0.linear1.weight', 'pl_model.autoencoder.decoder.decoder.layers.0.linear1.bias', 'pl_model.autoencoder.decoder.decoder.layers.0.linear2.weight', 'pl_model.autoencoder.decoder.decoder.layers.0.linear2.bias', 'pl_model.autoencoder.decoder.decoder.layers.0.norm1.weight', 'pl_model.autoencoder.decoder.decoder.layers.0.norm1.bias', 'pl_model.autoencoder.decoder.decoder.layers.0.norm2.weight', 'pl_model.autoencoder.decoder.decoder.layers.0.norm2.bias', 'pl_model.autoencoder.decoder.decoder.layers.0.norm3.weight', 'pl_model.autoencoder.decoder.decoder.layers.0.norm3.bias', 'pl_model.autoencoder.decoder.decoder.layers.1.self_attn.in_proj_weight', 'pl_model.autoencoder.decoder.decoder.layers.1.self_attn.in_proj_bias', 'pl_model.autoencoder.decoder.decoder.layers.1.self_attn.out_proj.weight', 'pl_model.autoencoder.decoder.decoder.layers.1.self_attn.out_proj.bias', 'pl_model.autoencoder.decoder.decoder.layers.1.multihead_attn.in_proj_weight', 'pl_model.autoencoder.decoder.decoder.layers.1.multihead_attn.in_proj_bias', 'pl_model.autoencoder.decoder.decoder.layers.1.multihead_attn.out_proj.weight', 'pl_model.autoencoder.decoder.decoder.layers.1.multihead_attn.out_proj.bias', 'pl_model.autoencoder.decoder.decoder.layers.1.linear1.weight', 'pl_model.autoencoder.decoder.decoder.layers.1.linear1.bias', 'pl_model.autoencoder.decoder.decoder.layers.1.linear2.weight', 'pl_model.autoencoder.decoder.decoder.layers.1.linear2.bias', 'pl_model.autoencoder.decoder.decoder.layers.1.norm1.weight', 'pl_model.autoencoder.decoder.decoder.layers.1.norm1.bias', 'pl_model.autoencoder.decoder.decoder.layers.1.norm2.weight', 'pl_model.autoencoder.decoder.decoder.layers.1.norm2.bias', 'pl_model.autoencoder.decoder.decoder.layers.1.norm3.weight', 'pl_model.autoencoder.decoder.decoder.layers.1.norm3.bias', 'pl_model.autoencoder.decoder.decoder.layers.2.self_attn.in_proj_weight', 'pl_model.autoencoder.decoder.decoder.layers.2.self_attn.in_proj_bias', 'pl_model.autoencoder.decoder.decoder.layers.2.self_attn.out_proj.weight', 'pl_model.autoencoder.decoder.decoder.layers.2.self_attn.out_proj.bias', 'pl_model.autoencoder.decoder.decoder.layers.2.multihead_attn.in_proj_weight', 'pl_model.autoencoder.decoder.decoder.layers.2.multihead_attn.in_proj_bias', 'pl_model.autoencoder.decoder.decoder.layers.2.multihead_attn.out_proj.weight', 'pl_model.autoencoder.decoder.decoder.layers.2.multihead_attn.out_proj.bias', 'pl_model.autoencoder.decoder.decoder.layers.2.linear1.weight', 'pl_model.autoencoder.decoder.decoder.layers.2.linear1.bias', 'pl_model.autoencoder.decoder.decoder.layers.2.linear2.weight', 'pl_model.autoencoder.decoder.decoder.layers.2.linear2.bias', 'pl_model.autoencoder.decoder.decoder.layers.2.norm1.weight', 'pl_model.autoencoder.decoder.decoder.layers.2.norm1.bias', 'pl_model.autoencoder.decoder.decoder.layers.2.norm2.weight', 'pl_model.autoencoder.decoder.decoder.layers.2.norm2.bias', 'pl_model.autoencoder.decoder.decoder.layers.2.norm3.weight', 'pl_model.autoencoder.decoder.decoder.layers.2.norm3.bias', 'pl_model.autoencoder.decoder.ln.weight', 'pl_model.autoencoder.decoder.ln.bias', 'pl_model.autoencoder.action_head.weight', 'pl_model.autoencoder.action_head.bias', 'pl_model.z_up.weight', 'pl_model.z_up.bias', 'pl_model.denoiser.encoder.layers.0.self_attn.in_proj_weight', 'pl_model.denoiser.encoder.layers.0.self_attn.in_proj_bias', 'pl_model.denoiser.encoder.layers.0.self_attn.out_proj.weight', 'pl_model.denoiser.encoder.layers.0.self_attn.out_proj.bias', 'pl_model.denoiser.encoder.layers.0.linear1.weight', 'pl_model.denoiser.encoder.layers.0.linear1.bias', 'pl_model.denoiser.encoder.layers.0.linear2.weight', 'pl_model.denoiser.encoder.layers.0.linear2.bias', 'pl_model.denoiser.encoder.layers.0.norm1.weight', 'pl_model.denoiser.encoder.layers.0.norm1.bias', 'pl_model.denoiser.encoder.layers.0.norm2.weight', 'pl_model.denoiser.encoder.layers.0.norm2.bias', 'pl_model.denoiser.encoder.layers.1.self_attn.in_proj_weight', 'pl_model.denoiser.encoder.layers.1.self_attn.in_proj_bias', 'pl_model.denoiser.encoder.layers.1.self_attn.out_proj.weight', 'pl_model.denoiser.encoder.layers.1.self_attn.out_proj.bias', 'pl_model.denoiser.encoder.layers.1.linear1.weight', 'pl_model.denoiser.encoder.layers.1.linear1.bias', 'pl_model.denoiser.encoder.layers.1.linear2.weight', 'pl_model.denoiser.encoder.layers.1.linear2.bias', 'pl_model.denoiser.encoder.layers.1.norm1.weight', 'pl_model.denoiser.encoder.layers.1.norm1.bias', 'pl_model.denoiser.encoder.layers.1.norm2.weight', 'pl_model.denoiser.encoder.layers.1.norm2.bias', 'pl_model.denoiser.encoder.layers.2.self_attn.in_proj_weight', 'pl_model.denoiser.encoder.layers.2.self_attn.in_proj_bias', 'pl_model.denoiser.encoder.layers.2.self_attn.out_proj.weight', 'pl_model.denoiser.encoder.layers.2.self_attn.out_proj.bias', 'pl_model.denoiser.encoder.layers.2.linear1.weight', 'pl_model.denoiser.encoder.layers.2.linear1.bias', 'pl_model.denoiser.encoder.layers.2.linear2.weight', 'pl_model.denoiser.encoder.layers.2.linear2.bias', 'pl_model.denoiser.encoder.layers.2.norm1.weight', 'pl_model.denoiser.encoder.layers.2.norm1.bias', 'pl_model.denoiser.encoder.layers.2.norm2.weight', 'pl_model.denoiser.encoder.layers.2.norm2.bias', 'pl_model.denoiser.encoder.layers.3.self_attn.in_proj_weight', 'pl_model.denoiser.encoder.layers.3.self_attn.in_proj_bias', 'pl_model.denoiser.encoder.layers.3.self_attn.out_proj.weight', 'pl_model.denoiser.encoder.layers.3.self_attn.out_proj.bias', 'pl_model.denoiser.encoder.layers.3.linear1.weight', 'pl_model.denoiser.encoder.layers.3.linear1.bias', 'pl_model.denoiser.encoder.layers.3.linear2.weight', 'pl_model.denoiser.encoder.layers.3.linear2.bias', 'pl_model.denoiser.encoder.layers.3.norm1.weight', 'pl_model.denoiser.encoder.layers.3.norm1.bias', 'pl_model.denoiser.encoder.layers.3.norm2.weight', 'pl_model.denoiser.encoder.layers.3.norm2.bias', 'pl_model.denoiser.encoder.layers.4.self_attn.in_proj_weight', 'pl_model.denoiser.encoder.layers.4.self_attn.in_proj_bias', 'pl_model.denoiser.encoder.layers.4.self_attn.out_proj.weight', 'pl_model.denoiser.encoder.layers.4.self_attn.out_proj.bias', 'pl_model.denoiser.encoder.layers.4.linear1.weight', 'pl_model.denoiser.encoder.layers.4.linear1.bias', 'pl_model.denoiser.encoder.layers.4.linear2.weight', 'pl_model.denoiser.encoder.layers.4.linear2.bias', 'pl_model.denoiser.encoder.layers.4.norm1.weight', 'pl_model.denoiser.encoder.layers.4.norm1.bias', 'pl_model.denoiser.encoder.layers.4.norm2.weight', 'pl_model.denoiser.encoder.layers.4.norm2.bias', 'pl_model.denoiser.encoder.layers.5.self_attn.in_proj_weight', 'pl_model.denoiser.encoder.layers.5.self_attn.in_proj_bias', 'pl_model.denoiser.encoder.layers.5.self_attn.out_proj.weight', 'pl_model.denoiser.encoder.layers.5.self_attn.out_proj.bias', 'pl_model.denoiser.encoder.layers.5.linear1.weight', 'pl_model.denoiser.encoder.layers.5.linear1.bias', 'pl_model.denoiser.encoder.layers.5.linear2.weight', 'pl_model.denoiser.encoder.layers.5.linear2.bias', 'pl_model.denoiser.encoder.layers.5.norm1.weight', 'pl_model.denoiser.encoder.layers.5.norm1.bias', 'pl_model.denoiser.encoder.layers.5.norm2.weight', 'pl_model.denoiser.encoder.layers.5.norm2.bias', 'pl_model.denoiser.ln.weight', 'pl_model.denoiser.ln.bias', 'pl_model.z_down.weight', 'pl_model.z_down.bias', 'pl_model.image_emb.weight', 'pl_model.image_emb.bias', 'pl_model.language_emb.weight', 'pl_model.language_emb.bias'])\n"
     ]
    }
   ],
   "source": [
    "# import pickle\n",
    "import torch\n",
    "import dill\n",
    "workspace_ckpt = '/home/zzy/robot/data/diffusion_policy_data/data/outputs/18.38.36_train_latent_diffusion_lift_image/checkpoints/latest.ckpt'\n",
    "with open(workspace_ckpt, 'rb') as f:\n",
    "    ckpt = torch.load(f, pickle_module=dill)\n",
    "\n",
    "print(ckpt['state_dicts']['model'].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['pe', 'autoencoder.cls', 'autoencoder.pe', 'autoencoder.action_emb.weight', 'autoencoder.action_emb.bias', 'autoencoder.z_encoder.encoder.layers.0.self_attn.in_proj_weight', 'autoencoder.z_encoder.encoder.layers.0.self_attn.in_proj_bias', 'autoencoder.z_encoder.encoder.layers.0.self_attn.out_proj.weight', 'autoencoder.z_encoder.encoder.layers.0.self_attn.out_proj.bias', 'autoencoder.z_encoder.encoder.layers.0.linear1.weight', 'autoencoder.z_encoder.encoder.layers.0.linear1.bias', 'autoencoder.z_encoder.encoder.layers.0.linear2.weight', 'autoencoder.z_encoder.encoder.layers.0.linear2.bias', 'autoencoder.z_encoder.encoder.layers.0.norm1.weight', 'autoencoder.z_encoder.encoder.layers.0.norm1.bias', 'autoencoder.z_encoder.encoder.layers.0.norm2.weight', 'autoencoder.z_encoder.encoder.layers.0.norm2.bias', 'autoencoder.z_encoder.encoder.layers.1.self_attn.in_proj_weight', 'autoencoder.z_encoder.encoder.layers.1.self_attn.in_proj_bias', 'autoencoder.z_encoder.encoder.layers.1.self_attn.out_proj.weight', 'autoencoder.z_encoder.encoder.layers.1.self_attn.out_proj.bias', 'autoencoder.z_encoder.encoder.layers.1.linear1.weight', 'autoencoder.z_encoder.encoder.layers.1.linear1.bias', 'autoencoder.z_encoder.encoder.layers.1.linear2.weight', 'autoencoder.z_encoder.encoder.layers.1.linear2.bias', 'autoencoder.z_encoder.encoder.layers.1.norm1.weight', 'autoencoder.z_encoder.encoder.layers.1.norm1.bias', 'autoencoder.z_encoder.encoder.layers.1.norm2.weight', 'autoencoder.z_encoder.encoder.layers.1.norm2.bias', 'autoencoder.z_encoder.encoder.layers.2.self_attn.in_proj_weight', 'autoencoder.z_encoder.encoder.layers.2.self_attn.in_proj_bias', 'autoencoder.z_encoder.encoder.layers.2.self_attn.out_proj.weight', 'autoencoder.z_encoder.encoder.layers.2.self_attn.out_proj.bias', 'autoencoder.z_encoder.encoder.layers.2.linear1.weight', 'autoencoder.z_encoder.encoder.layers.2.linear1.bias', 'autoencoder.z_encoder.encoder.layers.2.linear2.weight', 'autoencoder.z_encoder.encoder.layers.2.linear2.bias', 'autoencoder.z_encoder.encoder.layers.2.norm1.weight', 'autoencoder.z_encoder.encoder.layers.2.norm1.bias', 'autoencoder.z_encoder.encoder.layers.2.norm2.weight', 'autoencoder.z_encoder.encoder.layers.2.norm2.bias', 'autoencoder.z_encoder.ln.weight', 'autoencoder.z_encoder.ln.bias', 'autoencoder.z_down.weight', 'autoencoder.z_down.bias', 'autoencoder.z_up.weight', 'autoencoder.z_up.bias', 'autoencoder.conditioner.encoder.layers.0.self_attn.in_proj_weight', 'autoencoder.conditioner.encoder.layers.0.self_attn.in_proj_bias', 'autoencoder.conditioner.encoder.layers.0.self_attn.out_proj.weight', 'autoencoder.conditioner.encoder.layers.0.self_attn.out_proj.bias', 'autoencoder.conditioner.encoder.layers.0.linear1.weight', 'autoencoder.conditioner.encoder.layers.0.linear1.bias', 'autoencoder.conditioner.encoder.layers.0.linear2.weight', 'autoencoder.conditioner.encoder.layers.0.linear2.bias', 'autoencoder.conditioner.encoder.layers.0.norm1.weight', 'autoencoder.conditioner.encoder.layers.0.norm1.bias', 'autoencoder.conditioner.encoder.layers.0.norm2.weight', 'autoencoder.conditioner.encoder.layers.0.norm2.bias', 'autoencoder.conditioner.encoder.layers.1.self_attn.in_proj_weight', 'autoencoder.conditioner.encoder.layers.1.self_attn.in_proj_bias', 'autoencoder.conditioner.encoder.layers.1.self_attn.out_proj.weight', 'autoencoder.conditioner.encoder.layers.1.self_attn.out_proj.bias', 'autoencoder.conditioner.encoder.layers.1.linear1.weight', 'autoencoder.conditioner.encoder.layers.1.linear1.bias', 'autoencoder.conditioner.encoder.layers.1.linear2.weight', 'autoencoder.conditioner.encoder.layers.1.linear2.bias', 'autoencoder.conditioner.encoder.layers.1.norm1.weight', 'autoencoder.conditioner.encoder.layers.1.norm1.bias', 'autoencoder.conditioner.encoder.layers.1.norm2.weight', 'autoencoder.conditioner.encoder.layers.1.norm2.bias', 'autoencoder.conditioner.encoder.layers.2.self_attn.in_proj_weight', 'autoencoder.conditioner.encoder.layers.2.self_attn.in_proj_bias', 'autoencoder.conditioner.encoder.layers.2.self_attn.out_proj.weight', 'autoencoder.conditioner.encoder.layers.2.self_attn.out_proj.bias', 'autoencoder.conditioner.encoder.layers.2.linear1.weight', 'autoencoder.conditioner.encoder.layers.2.linear1.bias', 'autoencoder.conditioner.encoder.layers.2.linear2.weight', 'autoencoder.conditioner.encoder.layers.2.linear2.bias', 'autoencoder.conditioner.encoder.layers.2.norm1.weight', 'autoencoder.conditioner.encoder.layers.2.norm1.bias', 'autoencoder.conditioner.encoder.layers.2.norm2.weight', 'autoencoder.conditioner.encoder.layers.2.norm2.bias', 'autoencoder.conditioner.ln.weight', 'autoencoder.conditioner.ln.bias', 'autoencoder.decoder.decoder.layers.0.self_attn.in_proj_weight', 'autoencoder.decoder.decoder.layers.0.self_attn.in_proj_bias', 'autoencoder.decoder.decoder.layers.0.self_attn.out_proj.weight', 'autoencoder.decoder.decoder.layers.0.self_attn.out_proj.bias', 'autoencoder.decoder.decoder.layers.0.multihead_attn.in_proj_weight', 'autoencoder.decoder.decoder.layers.0.multihead_attn.in_proj_bias', 'autoencoder.decoder.decoder.layers.0.multihead_attn.out_proj.weight', 'autoencoder.decoder.decoder.layers.0.multihead_attn.out_proj.bias', 'autoencoder.decoder.decoder.layers.0.linear1.weight', 'autoencoder.decoder.decoder.layers.0.linear1.bias', 'autoencoder.decoder.decoder.layers.0.linear2.weight', 'autoencoder.decoder.decoder.layers.0.linear2.bias', 'autoencoder.decoder.decoder.layers.0.norm1.weight', 'autoencoder.decoder.decoder.layers.0.norm1.bias', 'autoencoder.decoder.decoder.layers.0.norm2.weight', 'autoencoder.decoder.decoder.layers.0.norm2.bias', 'autoencoder.decoder.decoder.layers.0.norm3.weight', 'autoencoder.decoder.decoder.layers.0.norm3.bias', 'autoencoder.decoder.decoder.layers.1.self_attn.in_proj_weight', 'autoencoder.decoder.decoder.layers.1.self_attn.in_proj_bias', 'autoencoder.decoder.decoder.layers.1.self_attn.out_proj.weight', 'autoencoder.decoder.decoder.layers.1.self_attn.out_proj.bias', 'autoencoder.decoder.decoder.layers.1.multihead_attn.in_proj_weight', 'autoencoder.decoder.decoder.layers.1.multihead_attn.in_proj_bias', 'autoencoder.decoder.decoder.layers.1.multihead_attn.out_proj.weight', 'autoencoder.decoder.decoder.layers.1.multihead_attn.out_proj.bias', 'autoencoder.decoder.decoder.layers.1.linear1.weight', 'autoencoder.decoder.decoder.layers.1.linear1.bias', 'autoencoder.decoder.decoder.layers.1.linear2.weight', 'autoencoder.decoder.decoder.layers.1.linear2.bias', 'autoencoder.decoder.decoder.layers.1.norm1.weight', 'autoencoder.decoder.decoder.layers.1.norm1.bias', 'autoencoder.decoder.decoder.layers.1.norm2.weight', 'autoencoder.decoder.decoder.layers.1.norm2.bias', 'autoencoder.decoder.decoder.layers.1.norm3.weight', 'autoencoder.decoder.decoder.layers.1.norm3.bias', 'autoencoder.decoder.decoder.layers.2.self_attn.in_proj_weight', 'autoencoder.decoder.decoder.layers.2.self_attn.in_proj_bias', 'autoencoder.decoder.decoder.layers.2.self_attn.out_proj.weight', 'autoencoder.decoder.decoder.layers.2.self_attn.out_proj.bias', 'autoencoder.decoder.decoder.layers.2.multihead_attn.in_proj_weight', 'autoencoder.decoder.decoder.layers.2.multihead_attn.in_proj_bias', 'autoencoder.decoder.decoder.layers.2.multihead_attn.out_proj.weight', 'autoencoder.decoder.decoder.layers.2.multihead_attn.out_proj.bias', 'autoencoder.decoder.decoder.layers.2.linear1.weight', 'autoencoder.decoder.decoder.layers.2.linear1.bias', 'autoencoder.decoder.decoder.layers.2.linear2.weight', 'autoencoder.decoder.decoder.layers.2.linear2.bias', 'autoencoder.decoder.decoder.layers.2.norm1.weight', 'autoencoder.decoder.decoder.layers.2.norm1.bias', 'autoencoder.decoder.decoder.layers.2.norm2.weight', 'autoencoder.decoder.decoder.layers.2.norm2.bias', 'autoencoder.decoder.decoder.layers.2.norm3.weight', 'autoencoder.decoder.decoder.layers.2.norm3.bias', 'autoencoder.decoder.ln.weight', 'autoencoder.decoder.ln.bias', 'autoencoder.action_head.weight', 'autoencoder.action_head.bias', 'z_up.weight', 'z_up.bias', 'denoiser.encoder.layers.0.self_attn.in_proj_weight', 'denoiser.encoder.layers.0.self_attn.in_proj_bias', 'denoiser.encoder.layers.0.self_attn.out_proj.weight', 'denoiser.encoder.layers.0.self_attn.out_proj.bias', 'denoiser.encoder.layers.0.linear1.weight', 'denoiser.encoder.layers.0.linear1.bias', 'denoiser.encoder.layers.0.linear2.weight', 'denoiser.encoder.layers.0.linear2.bias', 'denoiser.encoder.layers.0.norm1.weight', 'denoiser.encoder.layers.0.norm1.bias', 'denoiser.encoder.layers.0.norm2.weight', 'denoiser.encoder.layers.0.norm2.bias', 'denoiser.encoder.layers.1.self_attn.in_proj_weight', 'denoiser.encoder.layers.1.self_attn.in_proj_bias', 'denoiser.encoder.layers.1.self_attn.out_proj.weight', 'denoiser.encoder.layers.1.self_attn.out_proj.bias', 'denoiser.encoder.layers.1.linear1.weight', 'denoiser.encoder.layers.1.linear1.bias', 'denoiser.encoder.layers.1.linear2.weight', 'denoiser.encoder.layers.1.linear2.bias', 'denoiser.encoder.layers.1.norm1.weight', 'denoiser.encoder.layers.1.norm1.bias', 'denoiser.encoder.layers.1.norm2.weight', 'denoiser.encoder.layers.1.norm2.bias', 'denoiser.encoder.layers.2.self_attn.in_proj_weight', 'denoiser.encoder.layers.2.self_attn.in_proj_bias', 'denoiser.encoder.layers.2.self_attn.out_proj.weight', 'denoiser.encoder.layers.2.self_attn.out_proj.bias', 'denoiser.encoder.layers.2.linear1.weight', 'denoiser.encoder.layers.2.linear1.bias', 'denoiser.encoder.layers.2.linear2.weight', 'denoiser.encoder.layers.2.linear2.bias', 'denoiser.encoder.layers.2.norm1.weight', 'denoiser.encoder.layers.2.norm1.bias', 'denoiser.encoder.layers.2.norm2.weight', 'denoiser.encoder.layers.2.norm2.bias', 'denoiser.encoder.layers.3.self_attn.in_proj_weight', 'denoiser.encoder.layers.3.self_attn.in_proj_bias', 'denoiser.encoder.layers.3.self_attn.out_proj.weight', 'denoiser.encoder.layers.3.self_attn.out_proj.bias', 'denoiser.encoder.layers.3.linear1.weight', 'denoiser.encoder.layers.3.linear1.bias', 'denoiser.encoder.layers.3.linear2.weight', 'denoiser.encoder.layers.3.linear2.bias', 'denoiser.encoder.layers.3.norm1.weight', 'denoiser.encoder.layers.3.norm1.bias', 'denoiser.encoder.layers.3.norm2.weight', 'denoiser.encoder.layers.3.norm2.bias', 'denoiser.encoder.layers.4.self_attn.in_proj_weight', 'denoiser.encoder.layers.4.self_attn.in_proj_bias', 'denoiser.encoder.layers.4.self_attn.out_proj.weight', 'denoiser.encoder.layers.4.self_attn.out_proj.bias', 'denoiser.encoder.layers.4.linear1.weight', 'denoiser.encoder.layers.4.linear1.bias', 'denoiser.encoder.layers.4.linear2.weight', 'denoiser.encoder.layers.4.linear2.bias', 'denoiser.encoder.layers.4.norm1.weight', 'denoiser.encoder.layers.4.norm1.bias', 'denoiser.encoder.layers.4.norm2.weight', 'denoiser.encoder.layers.4.norm2.bias', 'denoiser.encoder.layers.5.self_attn.in_proj_weight', 'denoiser.encoder.layers.5.self_attn.in_proj_bias', 'denoiser.encoder.layers.5.self_attn.out_proj.weight', 'denoiser.encoder.layers.5.self_attn.out_proj.bias', 'denoiser.encoder.layers.5.linear1.weight', 'denoiser.encoder.layers.5.linear1.bias', 'denoiser.encoder.layers.5.linear2.weight', 'denoiser.encoder.layers.5.linear2.bias', 'denoiser.encoder.layers.5.norm1.weight', 'denoiser.encoder.layers.5.norm1.bias', 'denoiser.encoder.layers.5.norm2.weight', 'denoiser.encoder.layers.5.norm2.bias', 'denoiser.ln.weight', 'denoiser.ln.bias', 'z_down.weight', 'z_down.bias', 'image_emb.weight', 'image_emb.bias', 'language_emb.weight', 'language_emb.bias'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "pl_model_ckpt = '/home/zzy/robot/data/diffusion_policy_data/data/latent_diffusion_policy_ldm/em809a0c/checkpoints/epoch=1049-step=8400.ckpt'\n",
    "pl_model_ckpt = torch.load(pl_model_ckpt, pickle_module=dill)\n",
    "print(pl_model_ckpt['state_dict'].keys())\n",
    "for key in pl_model_ckpt['state_dict'].keys():\n",
    "    ckpt['state_dicts']['model'][f'pl_model.{key}'] = pl_model_ckpt['state_dict'][key]\n",
    "\n",
    "torch.save(ckpt, workspace_ckpt, pickle_module=dill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "import torch\n",
    "import dill\n",
    "import omegaconf\n",
    "workspace_ckpt = '/home/zzy/robot/data/diffusion_policy_data/data/outputs/18.38.36_train_latent_diffusion_lift_image/checkpoints/latest.ckpt'\n",
    "with open(workspace_ckpt, 'rb') as f:\n",
    "    ckpt = torch.load(f, pickle_module=dill)\n",
    "\n",
    "# print(ckpt['cfg']['ae_path'])\n",
    "# ckpt['cfg']['ae_path'] = '/home/zzy/robot/data/diffusion_policy_data/data/latent_diffusion_policy_ae/swkx352y/checkpoints/last.ckpt'\n",
    "# ckpt['cfg']['policy']['ae_path'] = '/home/zzy/robot/data/diffusion_policy_data/data/latent_diffusion_policy_ae/swkx352y/checkpoints/last.ckpt'\n",
    "# with omegaconf.flag_override(ckpt['cfg'], \"struct\", False):\n",
    "#     ckpt['cfg'].test_ae_only = True\n",
    "#     ckpt['cfg']['trainer']['logger'].name = 'xxx'\n",
    "#     ckpt['cfg']['trainer']['callbacks'][0].dirpath = 'xxx'\n",
    "# d = ckpt['cfg']\n",
    "# def change_path(d):\n",
    "#     if isinstance(d, omegaconf.dictconfig.DictConfig):\n",
    "#         for key, value in d.items():\n",
    "#             if isinstance(value, str):\n",
    "#                 value = value.replace('/home/zzy', '/home/zzy')\n",
    "#             else:\n",
    "#                 change_path(value)\n",
    "#     return d\n",
    "# d = change_path(d)\n",
    "# d = {'name': 'train_latent_diffusion', '_target_': 'diffusion_policy.workspace.train_latent_diffusion_workspace.TrainLatentDiffusionWorkspace', 'task_name': 'lift_image', 'shape_meta': {'obs': {'agentview_image': {'shape': [3, 84, 84], 'type': 'rgb'}, 'robot0_eye_in_hand_image': {'shape': [3, 84, 84], 'type': 'rgb'}, 'robot0_eef_pos': {'shape': [3]}, 'robot0_eef_quat': {'shape': [4]}, 'robot0_gripper_qpos': {'shape': [2]}}, 'action': {'shape': [10]}}, 'exp_name': 'default', 'ae_path': '/home/zzy/robot/data/diffusion_policy_data/data/latent_diffusion_policy_ae/swkx352y/checkpoints/last.ckpt', 'seed': 3407, 'horizon': 16, 'n_obs_steps': 1, 'n_action_steps': 8, 'n_latency_steps': 0, 'dataset_obs_steps': 1, 'past_action_visible': False, 'keypoint_visible_rate': 1.0, 'obs_as_global_cond': True, 'policy': {'_target_': 'diffusion_policy.policy.latent_diffusion_policy.LatentDiffusionPolicy', 'ae_path': '/home/zzy/robot/data/diffusion_policy_data/data/latent_diffusion_policy_ae/swkx352y/checkpoints/last.ckpt', 'shape_meta': {'obs': {'agentview_image': {'shape': [3, 84, 84], 'type': 'rgb'}, 'robot0_eye_in_hand_image': {'shape': [3, 84, 84], 'type': 'rgb'}, 'robot0_eef_pos': {'shape': [3]}, 'robot0_eef_quat': {'shape': [4]}, 'robot0_gripper_qpos': {'shape': [2]}}, 'action': {'shape': [10]}}, 'use_lr_scheduler': True, 'num_train_timesteps': 100, 'beta_start': 0.0001, 'beta_end': 0.02, 'num_inference_timesteps': 1000, 'beta_schedule': 'squaredcos_cap_v2', 'variance_type': 'fixed_small', 'clip_sample': True, 'prediction_type': 'epsilon', 'lr': 0.002, 'n_layers': 6, 'n_heads': 8, 'horizon': 16, 'dropout': 0.1}, 'datamodule': {'_target_': 'diffusion_policy.dataset.robomimic_replay_image_dataset.RobomimicImageDatamodule', 'batch_size': 1024, 'num_workers': 8, 'dataset': {'_target_': 'diffusion_policy.dataset.robomimic_replay_image_dataset.RobomimicReplayImageLanguageDataset', 'shape_meta': {'obs': {'agentview_image': {'shape': [3, 84, 84], 'type': 'rgb'}, 'robot0_eye_in_hand_image': {'shape': [3, 84, 84], 'type': 'rgb'}, 'robot0_eef_pos': {'shape': [3]}, 'robot0_eef_quat': {'shape': [4]}, 'robot0_gripper_qpos': {'shape': [2]}}, 'action': {'shape': [10]}}, 'dataset_path': '/home/zzy/robot/data/diffusion_policy_data/data/robomimic/datasets/lift/ph/image_abs.hdf5', 'horizon': 16, 'pad_before': 0, 'pad_after': 7, 'n_obs_steps': 1, 'abs_action': True, 'rotation_rep': 'rotation_6d', 'use_legacy_normalizer': False, 'use_cache': True, 'seed': 42, 'val_ratio': 0.02}}, 'trainer': {'_target_': 'lightning.pytorch.trainer.Trainer', 'max_epochs': 2000, 'check_val_every_n_epoch': 10, 'log_every_n_steps': 1, 'logger': {'_target_': 'lightning.pytorch.loggers.wandb.WandbLogger', 'project': 'latent_diffusion_policy_ldm', 'save_dir': '/home/zzy/robot/data/diffusion_policy_data/data/latent_dp_logs/', 'name': 'xxx'}, 'num_sanity_val_steps': 2, 'default_root_dir': '/home/zzy/robot/data/diffusion_policy_data/data/latent_dp_logs/', 'gradient_clip_val': 0.1, 'callbacks': [{'_target_': 'lightning.pytorch.callbacks.ModelCheckpoint', 'save_top_k': 1, 'save_last': True, 'save_weights_only': False, 'monitor': 'val/denoise_loss', 'mode': 'min', 'dirpath': 'xxx'}, {'_target_': 'lightning.pytorch.callbacks.TQDMProgressBar'}]}, 'training': {'use_ema': False}, 'logging': {'project': 'diffusion_policy_debug', 'resume': True, 'mode': 'online', 'name': '2024.07.31-18.38.36_train_latent_diffusion_lift_image', 'tags': ['train_latent_diffusion', 'lift_image', 'default'], 'id': None, 'group': None}, 'checkpoint': {'topk': {'monitor_key': 'test_mean_score', 'mode': 'max', 'k': 5, 'format_str': 'epoch={epoch:04d}-test_mean_score={test_mean_score:.3f}.ckpt'}, 'save_last_ckpt': True, 'save_last_snapshot': False}, 'multi_run': {'run_dir': '/home/zzy/robot/data/diffusion_policy_data/data/data/outputs/2024.07.31/18.38.36_train_latent_diffusion_lift_image', 'wandb_name_base': '2024.07.31-18.38.36_train_latent_diffusion_lift_image'}, 'task': {'name': 'lift_image', 'shape_meta': {'obs': {'agentview_image': {'shape': [3, 84, 84], 'type': 'rgb'}, 'robot0_eye_in_hand_image': {'shape': [3, 84, 84], 'type': 'rgb'}, 'robot0_eef_pos': {'shape': [3]}, 'robot0_eef_quat': {'shape': [4]}, 'robot0_gripper_qpos': {'shape': [2]}}, 'action': {'shape': [10]}}, 'task_name': 'lift', 'dataset_type': 'ph', 'dataset_path': '/home/zzy/robot/data/diffusion_policy_data/data/robomimic/datasets/lift/ph/image_abs.hdf5', 'abs_action': True, 'env_runner': {'_target_': 'diffusion_policy.env_runner.robomimic_image_runner.RobomimicImageRunner', 'dataset_path': '/home/zzy/robot/data/diffusion_policy_data/data/robomimic/datasets/lift/ph/image_abs.hdf5', 'shape_meta': {'obs': {'agentview_image': {'shape': [3, 84, 84], 'type': 'rgb'}, 'robot0_eye_in_hand_image': {'shape': [3, 84, 84], 'type': 'rgb'}, 'robot0_eef_pos': {'shape': [3]}, 'robot0_eef_quat': {'shape': [4]}, 'robot0_gripper_qpos': {'shape': [2]}}, 'action': {'shape': [10]}}, 'n_train': 1, 'n_train_vis': 2, 'train_start_idx': 0, 'n_test': 1, 'n_test_vis': 1, 'test_start_seed': 100000, 'max_steps': 400, 'n_obs_steps': 1, 'n_action_steps': 8, 'render_obs_key': 'agentview_image', 'fps': 10, 'crf': 22, 'past_action': False, 'abs_action': True, 'tqdm_interval_sec': 1.0, 'n_envs': 1}, 'dataset': {'_target_': 'diffusion_policy.dataset.robomimic_replay_image_dataset.RobomimicReplayImageLanguageDataset', 'shape_meta': {'obs': {'agentview_image': {'shape': [3, 84, 84], 'type': 'rgb'}, 'robot0_eye_in_hand_image': {'shape': [3, 84, 84], 'type': 'rgb'}, 'robot0_eef_pos': {'shape': [3]}, 'robot0_eef_quat': {'shape': [4]}, 'robot0_gripper_qpos': {'shape': [2]}}, 'action': {'shape': [10]}}, 'dataset_path': '/home/zzy/robot/data/diffusion_policy_data/data/robomimic/datasets/lift/ph/image_abs.hdf5', 'horizon': 16, 'pad_before': 0, 'pad_after': 7, 'n_obs_steps': 1, 'abs_action': True, 'rotation_rep': 'rotation_6d', 'use_legacy_normalizer': False, 'use_cache': True, 'seed': 42, 'val_ratio': 0.02}}, 'test_ae_only': True}\n",
    "# ckpt['cfg'] = omegaconf.OmegaConf.create(d)\n",
    "# print(d)\n",
    "ckpt['cfg']['test_ae_only'] = False\n",
    "torch.save(ckpt, workspace_ckpt, pickle_module=dill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['_dummy_variable', 'pl_model.cls', 'pl_model.pe', 'pl_model.action_emb.weight', 'pl_model.action_emb.bias', 'pl_model.z_encoder.encoder.layers.0.self_attn.in_proj_weight', 'pl_model.z_encoder.encoder.layers.0.self_attn.in_proj_bias', 'pl_model.z_encoder.encoder.layers.0.self_attn.out_proj.weight', 'pl_model.z_encoder.encoder.layers.0.self_attn.out_proj.bias', 'pl_model.z_encoder.encoder.layers.0.linear1.weight', 'pl_model.z_encoder.encoder.layers.0.linear1.bias', 'pl_model.z_encoder.encoder.layers.0.linear2.weight', 'pl_model.z_encoder.encoder.layers.0.linear2.bias', 'pl_model.z_encoder.encoder.layers.0.norm1.weight', 'pl_model.z_encoder.encoder.layers.0.norm1.bias', 'pl_model.z_encoder.encoder.layers.0.norm2.weight', 'pl_model.z_encoder.encoder.layers.0.norm2.bias', 'pl_model.z_encoder.encoder.layers.1.self_attn.in_proj_weight', 'pl_model.z_encoder.encoder.layers.1.self_attn.in_proj_bias', 'pl_model.z_encoder.encoder.layers.1.self_attn.out_proj.weight', 'pl_model.z_encoder.encoder.layers.1.self_attn.out_proj.bias', 'pl_model.z_encoder.encoder.layers.1.linear1.weight', 'pl_model.z_encoder.encoder.layers.1.linear1.bias', 'pl_model.z_encoder.encoder.layers.1.linear2.weight', 'pl_model.z_encoder.encoder.layers.1.linear2.bias', 'pl_model.z_encoder.encoder.layers.1.norm1.weight', 'pl_model.z_encoder.encoder.layers.1.norm1.bias', 'pl_model.z_encoder.encoder.layers.1.norm2.weight', 'pl_model.z_encoder.encoder.layers.1.norm2.bias', 'pl_model.z_encoder.encoder.layers.2.self_attn.in_proj_weight', 'pl_model.z_encoder.encoder.layers.2.self_attn.in_proj_bias', 'pl_model.z_encoder.encoder.layers.2.self_attn.out_proj.weight', 'pl_model.z_encoder.encoder.layers.2.self_attn.out_proj.bias', 'pl_model.z_encoder.encoder.layers.2.linear1.weight', 'pl_model.z_encoder.encoder.layers.2.linear1.bias', 'pl_model.z_encoder.encoder.layers.2.linear2.weight', 'pl_model.z_encoder.encoder.layers.2.linear2.bias', 'pl_model.z_encoder.encoder.layers.2.norm1.weight', 'pl_model.z_encoder.encoder.layers.2.norm1.bias', 'pl_model.z_encoder.encoder.layers.2.norm2.weight', 'pl_model.z_encoder.encoder.layers.2.norm2.bias', 'pl_model.z_encoder.ln.weight', 'pl_model.z_encoder.ln.bias', 'pl_model.z_down.weight', 'pl_model.z_down.bias', 'pl_model.z_up.weight', 'pl_model.z_up.bias', 'pl_model.conditioner.encoder.layers.0.self_attn.in_proj_weight', 'pl_model.conditioner.encoder.layers.0.self_attn.in_proj_bias', 'pl_model.conditioner.encoder.layers.0.self_attn.out_proj.weight', 'pl_model.conditioner.encoder.layers.0.self_attn.out_proj.bias', 'pl_model.conditioner.encoder.layers.0.linear1.weight', 'pl_model.conditioner.encoder.layers.0.linear1.bias', 'pl_model.conditioner.encoder.layers.0.linear2.weight', 'pl_model.conditioner.encoder.layers.0.linear2.bias', 'pl_model.conditioner.encoder.layers.0.norm1.weight', 'pl_model.conditioner.encoder.layers.0.norm1.bias', 'pl_model.conditioner.encoder.layers.0.norm2.weight', 'pl_model.conditioner.encoder.layers.0.norm2.bias', 'pl_model.conditioner.encoder.layers.1.self_attn.in_proj_weight', 'pl_model.conditioner.encoder.layers.1.self_attn.in_proj_bias', 'pl_model.conditioner.encoder.layers.1.self_attn.out_proj.weight', 'pl_model.conditioner.encoder.layers.1.self_attn.out_proj.bias', 'pl_model.conditioner.encoder.layers.1.linear1.weight', 'pl_model.conditioner.encoder.layers.1.linear1.bias', 'pl_model.conditioner.encoder.layers.1.linear2.weight', 'pl_model.conditioner.encoder.layers.1.linear2.bias', 'pl_model.conditioner.encoder.layers.1.norm1.weight', 'pl_model.conditioner.encoder.layers.1.norm1.bias', 'pl_model.conditioner.encoder.layers.1.norm2.weight', 'pl_model.conditioner.encoder.layers.1.norm2.bias', 'pl_model.conditioner.encoder.layers.2.self_attn.in_proj_weight', 'pl_model.conditioner.encoder.layers.2.self_attn.in_proj_bias', 'pl_model.conditioner.encoder.layers.2.self_attn.out_proj.weight', 'pl_model.conditioner.encoder.layers.2.self_attn.out_proj.bias', 'pl_model.conditioner.encoder.layers.2.linear1.weight', 'pl_model.conditioner.encoder.layers.2.linear1.bias', 'pl_model.conditioner.encoder.layers.2.linear2.weight', 'pl_model.conditioner.encoder.layers.2.linear2.bias', 'pl_model.conditioner.encoder.layers.2.norm1.weight', 'pl_model.conditioner.encoder.layers.2.norm1.bias', 'pl_model.conditioner.encoder.layers.2.norm2.weight', 'pl_model.conditioner.encoder.layers.2.norm2.bias', 'pl_model.conditioner.ln.weight', 'pl_model.conditioner.ln.bias', 'pl_model.decoder.decoder.layers.0.self_attn.in_proj_weight', 'pl_model.decoder.decoder.layers.0.self_attn.in_proj_bias', 'pl_model.decoder.decoder.layers.0.self_attn.out_proj.weight', 'pl_model.decoder.decoder.layers.0.self_attn.out_proj.bias', 'pl_model.decoder.decoder.layers.0.multihead_attn.in_proj_weight', 'pl_model.decoder.decoder.layers.0.multihead_attn.in_proj_bias', 'pl_model.decoder.decoder.layers.0.multihead_attn.out_proj.weight', 'pl_model.decoder.decoder.layers.0.multihead_attn.out_proj.bias', 'pl_model.decoder.decoder.layers.0.linear1.weight', 'pl_model.decoder.decoder.layers.0.linear1.bias', 'pl_model.decoder.decoder.layers.0.linear2.weight', 'pl_model.decoder.decoder.layers.0.linear2.bias', 'pl_model.decoder.decoder.layers.0.norm1.weight', 'pl_model.decoder.decoder.layers.0.norm1.bias', 'pl_model.decoder.decoder.layers.0.norm2.weight', 'pl_model.decoder.decoder.layers.0.norm2.bias', 'pl_model.decoder.decoder.layers.0.norm3.weight', 'pl_model.decoder.decoder.layers.0.norm3.bias', 'pl_model.decoder.decoder.layers.1.self_attn.in_proj_weight', 'pl_model.decoder.decoder.layers.1.self_attn.in_proj_bias', 'pl_model.decoder.decoder.layers.1.self_attn.out_proj.weight', 'pl_model.decoder.decoder.layers.1.self_attn.out_proj.bias', 'pl_model.decoder.decoder.layers.1.multihead_attn.in_proj_weight', 'pl_model.decoder.decoder.layers.1.multihead_attn.in_proj_bias', 'pl_model.decoder.decoder.layers.1.multihead_attn.out_proj.weight', 'pl_model.decoder.decoder.layers.1.multihead_attn.out_proj.bias', 'pl_model.decoder.decoder.layers.1.linear1.weight', 'pl_model.decoder.decoder.layers.1.linear1.bias', 'pl_model.decoder.decoder.layers.1.linear2.weight', 'pl_model.decoder.decoder.layers.1.linear2.bias', 'pl_model.decoder.decoder.layers.1.norm1.weight', 'pl_model.decoder.decoder.layers.1.norm1.bias', 'pl_model.decoder.decoder.layers.1.norm2.weight', 'pl_model.decoder.decoder.layers.1.norm2.bias', 'pl_model.decoder.decoder.layers.1.norm3.weight', 'pl_model.decoder.decoder.layers.1.norm3.bias', 'pl_model.decoder.decoder.layers.2.self_attn.in_proj_weight', 'pl_model.decoder.decoder.layers.2.self_attn.in_proj_bias', 'pl_model.decoder.decoder.layers.2.self_attn.out_proj.weight', 'pl_model.decoder.decoder.layers.2.self_attn.out_proj.bias', 'pl_model.decoder.decoder.layers.2.multihead_attn.in_proj_weight', 'pl_model.decoder.decoder.layers.2.multihead_attn.in_proj_bias', 'pl_model.decoder.decoder.layers.2.multihead_attn.out_proj.weight', 'pl_model.decoder.decoder.layers.2.multihead_attn.out_proj.bias', 'pl_model.decoder.decoder.layers.2.linear1.weight', 'pl_model.decoder.decoder.layers.2.linear1.bias', 'pl_model.decoder.decoder.layers.2.linear2.weight', 'pl_model.decoder.decoder.layers.2.linear2.bias', 'pl_model.decoder.decoder.layers.2.norm1.weight', 'pl_model.decoder.decoder.layers.2.norm1.bias', 'pl_model.decoder.decoder.layers.2.norm2.weight', 'pl_model.decoder.decoder.layers.2.norm2.bias', 'pl_model.decoder.decoder.layers.2.norm3.weight', 'pl_model.decoder.decoder.layers.2.norm3.bias', 'pl_model.decoder.ln.weight', 'pl_model.decoder.ln.bias', 'pl_model.action_head.weight', 'pl_model.action_head.bias', 'pl_model.normalizer.params_dict.action.offset', 'pl_model.normalizer.params_dict.action.scale', 'pl_model.normalizer.params_dict.action.input_stats.max', 'pl_model.normalizer.params_dict.action.input_stats.mean', 'pl_model.normalizer.params_dict.action.input_stats.min', 'pl_model.normalizer.params_dict.action.input_stats.std', 'pl_model.normalizer.params_dict.obs.offset', 'pl_model.normalizer.params_dict.obs.scale', 'pl_model.normalizer.params_dict.obs.input_stats.max', 'pl_model.normalizer.params_dict.obs.input_stats.mean', 'pl_model.normalizer.params_dict.obs.input_stats.min', 'pl_model.normalizer.params_dict.obs.input_stats.std', 'normalizer.params_dict.action.offset', 'normalizer.params_dict.action.scale', 'normalizer.params_dict.action.input_stats.max', 'normalizer.params_dict.action.input_stats.mean', 'normalizer.params_dict.action.input_stats.min', 'normalizer.params_dict.action.input_stats.std', 'normalizer.params_dict.obs.offset', 'normalizer.params_dict.obs.scale', 'normalizer.params_dict.obs.input_stats.max', 'normalizer.params_dict.obs.input_stats.mean', 'normalizer.params_dict.obs.input_stats.min', 'normalizer.params_dict.obs.input_stats.std'])\n"
     ]
    }
   ],
   "source": [
    "import dill\n",
    "import torch    \n",
    "path = '/home/zzy/robot/data/diffusion_policy_data/data/outputs/2024.08.05/00.44.58_train_ae_lift_lowdim/checkpoints/latest.ckpt'\n",
    "with open(path, 'rb') as f:\n",
    "    ckpt = torch.load(f, pickle_module=dill)\n",
    "print(ckpt['state_dicts']['model'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['cls', 'pe', 'action_emb.weight', 'action_emb.bias', 'z_encoder.encoder.layers.0.self_attn.in_proj_weight', 'z_encoder.encoder.layers.0.self_attn.in_proj_bias', 'z_encoder.encoder.layers.0.self_attn.out_proj.weight', 'z_encoder.encoder.layers.0.self_attn.out_proj.bias', 'z_encoder.encoder.layers.0.linear1.weight', 'z_encoder.encoder.layers.0.linear1.bias', 'z_encoder.encoder.layers.0.linear2.weight', 'z_encoder.encoder.layers.0.linear2.bias', 'z_encoder.encoder.layers.0.norm1.weight', 'z_encoder.encoder.layers.0.norm1.bias', 'z_encoder.encoder.layers.0.norm2.weight', 'z_encoder.encoder.layers.0.norm2.bias', 'z_encoder.encoder.layers.1.self_attn.in_proj_weight', 'z_encoder.encoder.layers.1.self_attn.in_proj_bias', 'z_encoder.encoder.layers.1.self_attn.out_proj.weight', 'z_encoder.encoder.layers.1.self_attn.out_proj.bias', 'z_encoder.encoder.layers.1.linear1.weight', 'z_encoder.encoder.layers.1.linear1.bias', 'z_encoder.encoder.layers.1.linear2.weight', 'z_encoder.encoder.layers.1.linear2.bias', 'z_encoder.encoder.layers.1.norm1.weight', 'z_encoder.encoder.layers.1.norm1.bias', 'z_encoder.encoder.layers.1.norm2.weight', 'z_encoder.encoder.layers.1.norm2.bias', 'z_encoder.encoder.layers.2.self_attn.in_proj_weight', 'z_encoder.encoder.layers.2.self_attn.in_proj_bias', 'z_encoder.encoder.layers.2.self_attn.out_proj.weight', 'z_encoder.encoder.layers.2.self_attn.out_proj.bias', 'z_encoder.encoder.layers.2.linear1.weight', 'z_encoder.encoder.layers.2.linear1.bias', 'z_encoder.encoder.layers.2.linear2.weight', 'z_encoder.encoder.layers.2.linear2.bias', 'z_encoder.encoder.layers.2.norm1.weight', 'z_encoder.encoder.layers.2.norm1.bias', 'z_encoder.encoder.layers.2.norm2.weight', 'z_encoder.encoder.layers.2.norm2.bias', 'z_encoder.ln.weight', 'z_encoder.ln.bias', 'z_down.weight', 'z_down.bias', 'z_up.weight', 'z_up.bias', 'conditioner.encoder.layers.0.self_attn.in_proj_weight', 'conditioner.encoder.layers.0.self_attn.in_proj_bias', 'conditioner.encoder.layers.0.self_attn.out_proj.weight', 'conditioner.encoder.layers.0.self_attn.out_proj.bias', 'conditioner.encoder.layers.0.linear1.weight', 'conditioner.encoder.layers.0.linear1.bias', 'conditioner.encoder.layers.0.linear2.weight', 'conditioner.encoder.layers.0.linear2.bias', 'conditioner.encoder.layers.0.norm1.weight', 'conditioner.encoder.layers.0.norm1.bias', 'conditioner.encoder.layers.0.norm2.weight', 'conditioner.encoder.layers.0.norm2.bias', 'conditioner.encoder.layers.1.self_attn.in_proj_weight', 'conditioner.encoder.layers.1.self_attn.in_proj_bias', 'conditioner.encoder.layers.1.self_attn.out_proj.weight', 'conditioner.encoder.layers.1.self_attn.out_proj.bias', 'conditioner.encoder.layers.1.linear1.weight', 'conditioner.encoder.layers.1.linear1.bias', 'conditioner.encoder.layers.1.linear2.weight', 'conditioner.encoder.layers.1.linear2.bias', 'conditioner.encoder.layers.1.norm1.weight', 'conditioner.encoder.layers.1.norm1.bias', 'conditioner.encoder.layers.1.norm2.weight', 'conditioner.encoder.layers.1.norm2.bias', 'conditioner.encoder.layers.2.self_attn.in_proj_weight', 'conditioner.encoder.layers.2.self_attn.in_proj_bias', 'conditioner.encoder.layers.2.self_attn.out_proj.weight', 'conditioner.encoder.layers.2.self_attn.out_proj.bias', 'conditioner.encoder.layers.2.linear1.weight', 'conditioner.encoder.layers.2.linear1.bias', 'conditioner.encoder.layers.2.linear2.weight', 'conditioner.encoder.layers.2.linear2.bias', 'conditioner.encoder.layers.2.norm1.weight', 'conditioner.encoder.layers.2.norm1.bias', 'conditioner.encoder.layers.2.norm2.weight', 'conditioner.encoder.layers.2.norm2.bias', 'conditioner.ln.weight', 'conditioner.ln.bias', 'decoder.decoder.layers.0.self_attn.in_proj_weight', 'decoder.decoder.layers.0.self_attn.in_proj_bias', 'decoder.decoder.layers.0.self_attn.out_proj.weight', 'decoder.decoder.layers.0.self_attn.out_proj.bias', 'decoder.decoder.layers.0.multihead_attn.in_proj_weight', 'decoder.decoder.layers.0.multihead_attn.in_proj_bias', 'decoder.decoder.layers.0.multihead_attn.out_proj.weight', 'decoder.decoder.layers.0.multihead_attn.out_proj.bias', 'decoder.decoder.layers.0.linear1.weight', 'decoder.decoder.layers.0.linear1.bias', 'decoder.decoder.layers.0.linear2.weight', 'decoder.decoder.layers.0.linear2.bias', 'decoder.decoder.layers.0.norm1.weight', 'decoder.decoder.layers.0.norm1.bias', 'decoder.decoder.layers.0.norm2.weight', 'decoder.decoder.layers.0.norm2.bias', 'decoder.decoder.layers.0.norm3.weight', 'decoder.decoder.layers.0.norm3.bias', 'decoder.decoder.layers.1.self_attn.in_proj_weight', 'decoder.decoder.layers.1.self_attn.in_proj_bias', 'decoder.decoder.layers.1.self_attn.out_proj.weight', 'decoder.decoder.layers.1.self_attn.out_proj.bias', 'decoder.decoder.layers.1.multihead_attn.in_proj_weight', 'decoder.decoder.layers.1.multihead_attn.in_proj_bias', 'decoder.decoder.layers.1.multihead_attn.out_proj.weight', 'decoder.decoder.layers.1.multihead_attn.out_proj.bias', 'decoder.decoder.layers.1.linear1.weight', 'decoder.decoder.layers.1.linear1.bias', 'decoder.decoder.layers.1.linear2.weight', 'decoder.decoder.layers.1.linear2.bias', 'decoder.decoder.layers.1.norm1.weight', 'decoder.decoder.layers.1.norm1.bias', 'decoder.decoder.layers.1.norm2.weight', 'decoder.decoder.layers.1.norm2.bias', 'decoder.decoder.layers.1.norm3.weight', 'decoder.decoder.layers.1.norm3.bias', 'decoder.decoder.layers.2.self_attn.in_proj_weight', 'decoder.decoder.layers.2.self_attn.in_proj_bias', 'decoder.decoder.layers.2.self_attn.out_proj.weight', 'decoder.decoder.layers.2.self_attn.out_proj.bias', 'decoder.decoder.layers.2.multihead_attn.in_proj_weight', 'decoder.decoder.layers.2.multihead_attn.in_proj_bias', 'decoder.decoder.layers.2.multihead_attn.out_proj.weight', 'decoder.decoder.layers.2.multihead_attn.out_proj.bias', 'decoder.decoder.layers.2.linear1.weight', 'decoder.decoder.layers.2.linear1.bias', 'decoder.decoder.layers.2.linear2.weight', 'decoder.decoder.layers.2.linear2.bias', 'decoder.decoder.layers.2.norm1.weight', 'decoder.decoder.layers.2.norm1.bias', 'decoder.decoder.layers.2.norm2.weight', 'decoder.decoder.layers.2.norm2.bias', 'decoder.decoder.layers.2.norm3.weight', 'decoder.decoder.layers.2.norm3.bias', 'decoder.ln.weight', 'decoder.ln.bias', 'action_head.weight', 'action_head.bias', 'normalizer.params_dict.action.offset', 'normalizer.params_dict.action.scale', 'normalizer.params_dict.action.input_stats.max', 'normalizer.params_dict.action.input_stats.mean', 'normalizer.params_dict.action.input_stats.min', 'normalizer.params_dict.action.input_stats.std', 'normalizer.params_dict.obs.offset', 'normalizer.params_dict.obs.scale', 'normalizer.params_dict.obs.input_stats.max', 'normalizer.params_dict.obs.input_stats.mean', 'normalizer.params_dict.obs.input_stats.min', 'normalizer.params_dict.obs.input_stats.std'])\n"
     ]
    }
   ],
   "source": [
    "ae_path = '/home/zzy/robot/data/diffusion_policy_data/data/latent_dp_logs/latent_diffusion_policy_ae/xiqccmzm/checkpoints/last.ckpt'\n",
    "ae_ckpt = torch.load(ae_path, pickle_module=dill)\n",
    "print(ae_ckpt['state_dict'].keys())\n",
    "for key in ae_ckpt['state_dict'].keys():\n",
    "    ckpt['state_dicts']['model'][f'pl_model.{key}'] = ae_ckpt['state_dict'][key]\n",
    "torch.save(ckpt, path, pickle_module=dill)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diffusion_policy.env_runner.robomimic_lowdim_runner_ae.RobomimicLowdimRunnerAE\n"
     ]
    }
   ],
   "source": [
    "path = '/home/zzy/robot/data/diffusion_policy_data/data/outputs/2024.08.06/23.18.52_train_ae_square_lowdim/checkpoints/latest.ckpt'\n",
    "import torch\n",
    "import dill\n",
    "with open(path, 'rb') as f:\n",
    "    ckpt = torch.load(f, pickle_module=dill)\n",
    "print(ckpt['cfg']['task']['env_runner']._target_)\n",
    "ckpt['cfg']['task']['env_runner']._target_ = 'diffusion_policy.env_runner.robomimic_image_runner_ae.RobomimicImageRunnerAE'\n",
    "torch.save(ckpt, path, pickle_module=dill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zzy/robot/data/diffusion_policy_data/data/latent_diffusion_policy_ae/2024-08-07_17-54-46/last.ckpt\n",
      "{'name': 'train_latent_diffusion', '_target_': 'diffusion_policy.workspace.train_latent_diffusion_workspace.TrainLatentDiffusionWorkspace', 'task_name': 'can_image', 'shape_meta': {'obs': {'agentview_image': {'shape': [3, 84, 84], 'type': 'rgb'}, 'robot0_eye_in_hand_image': {'shape': [3, 84, 84], 'type': 'rgb'}, 'robot0_eef_pos': {'shape': [3]}, 'robot0_eef_quat': {'shape': [4]}, 'robot0_gripper_qpos': {'shape': [2]}}, 'action': {'shape': [10]}}, 'exp_name': 'default', 'resume_ckpt_path': None, 'ae_path': '/home/zzy/robot/data/diffusion_policy_data/data/latent_diffusion_policy_ae/2024-08-07_17-54-46/last.ckpt', 'seed': 3407, 'horizon': 16, 'n_obs_steps': 1, 'n_action_steps': 16, 'n_latency_steps': 0, 'dataset_obs_steps': 1, 'past_action_visible': False, 'keypoint_visible_rate': 1.0, 'obs_as_global_cond': True, 'load_image_features': True, 'policy': {'_target_': 'diffusion_policy.policy.latent_diffusion_policy.LatentDiffusionPolicy', 'ae_path': '/home/zzy/robot/data/diffusion_policy_data/data/latent_diffusion_policy_ae/2024-08-07_17-54-46/last.ckpt', 'image_feature_dim': 512, 'load_image_features': True, 'shape_meta': {'obs': {'agentview_image': {'shape': [3, 84, 84], 'type': 'rgb'}, 'robot0_eye_in_hand_image': {'shape': [3, 84, 84], 'type': 'rgb'}, 'robot0_eef_pos': {'shape': [3]}, 'robot0_eef_quat': {'shape': [4]}, 'robot0_gripper_qpos': {'shape': [2]}}, 'action': {'shape': [10]}}, 'num_train_timesteps': 1000, 'beta_start': 0.0001, 'beta_end': 0.02, 'num_inference_timesteps': 1000, 'beta_schedule': 'squaredcos_cap_v2', 'variance_type': 'fixed_small', 'clip_sample': True, 'prediction_type': 'epsilon', 'n_layers': 6, 'n_heads': 8, 'hidden_size': 1024, 'horizon': 16, 'dropout': 0.1, 'lr': 0.0002, 'use_lr_scheduler': True, 'warmup_steps': 1000, 'task_name': 'can_image'}, 'datamodule': {'_target_': 'diffusion_policy.dataset.robomimic_replay_image_dataset.RobomimicImageDatamodule', 'batch_size': 32768, 'num_workers': 4, 'dataset': {'_target_': 'diffusion_policy.dataset.robomimic_replay_image_dataset.RobomimicReplayImageLanguageDataset', 'shape_meta': {'obs': {'agentview_image': {'shape': [3, 84, 84], 'type': 'rgb'}, 'robot0_eye_in_hand_image': {'shape': [3, 84, 84], 'type': 'rgb'}, 'robot0_eef_pos': {'shape': [3]}, 'robot0_eef_quat': {'shape': [4]}, 'robot0_gripper_qpos': {'shape': [2]}}, 'action': {'shape': [10]}}, 'dataset_path': '/home/zzy/robotdata/diffusion_policy_data/data/robomimic/datasets/can/ph/image_abs.hdf5', 'horizon': 16, 'pad_before': 0, 'pad_after': 15, 'n_obs_steps': 1, 'abs_action': True, 'rotation_rep': 'rotation_6d', 'use_legacy_normalizer': False, 'use_cache': True, 'seed': 42, 'val_ratio': 0.02, 'load_image_features': True}}, 'trainer': {'_target_': 'lightning.pytorch.trainer.Trainer', 'max_epochs': 20000, 'max_steps': -1, 'precision': 'bf16-mixed', 'check_val_every_n_epoch': 50, 'log_every_n_steps': 1, 'strategy': 'ddp_find_unused_parameters_true', 'logger': {'_target_': 'lightning.pytorch.loggers.wandb.WandbLogger', 'project': 'latent_diffusion_policy_ldm', 'name': '2024-08-08_15-06-53', 'save_dir': '/home/zzy/robotdata/diffusion_policy_data/data/latent_diffusion_policy_ldm/2024-08-08_15-06-53'}, 'num_sanity_val_steps': 2, 'default_root_dir': '/home/zzy/robotdata/diffusion_policy_data/data/latent_dp_logs/', 'profiler': 'simple', 'gradient_clip_val': 1, 'callbacks': [{'_target_': 'lightning.pytorch.callbacks.ModelCheckpoint', 'save_top_k': 1, 'save_last': True, 'save_weights_only': False, 'monitor': 'val/denoise_loss', 'mode': 'min', 'dirpath': '/home/zzy/robotdata/diffusion_policy_data/data/latent_diffusion_policy_ldm/2024-08-08_15-06-53'}, {'_target_': 'lightning.pytorch.callbacks.TQDMProgressBar'}]}, 'training': {'use_ema': False}, 'logging': {'project': 'diffusion_policy_debug', 'resume': True, 'mode': 'online', 'name': '2024.08.08-15.06.50_train_latent_diffusion_can_image', 'tags': ['train_latent_diffusion', 'can_image', 'default'], 'id': None, 'group': None}, 'checkpoint': {'topk': {'monitor_key': 'test_mean_score', 'mode': 'max', 'k': 5, 'format_str': 'epoch={epoch:04d}-test_mean_score={test_mean_score:.3f}.ckpt'}, 'save_last_ckpt': True, 'save_last_snapshot': False}, 'multi_run': {'run_dir': '/home/zzy/robotdata/diffusion_policy_data/data/data/outputs/2024.08.08/15.06.50_train_latent_diffusion_can_image', 'wandb_name_base': '2024.08.08-15.06.50_train_latent_diffusion_can_image'}, 'task': {'name': 'can_image', 'shape_meta': {'obs': {'agentview_image': {'shape': [3, 84, 84], 'type': 'rgb'}, 'robot0_eye_in_hand_image': {'shape': [3, 84, 84], 'type': 'rgb'}, 'robot0_eef_pos': {'shape': [3]}, 'robot0_eef_quat': {'shape': [4]}, 'robot0_gripper_qpos': {'shape': [2]}}, 'action': {'shape': [10]}}, 'task_name': 'can', 'dataset_type': 'ph', 'dataset_path': '/home/zzy/robotdata/diffusion_policy_data/data/robomimic/datasets/can/ph/image_abs.hdf5', 'abs_action': True, 'env_runner': {'_target_': 'diffusion_policy.env_runner.robomimic_image_runner_ae.RobomimicImageRunnerAE', 'dataset_path': '/home/zzy/robotdata/diffusion_policy_data/data/robomimic/datasets/can/ph/image_abs.hdf5', 'shape_meta': {'obs': {'agentview_image': {'shape': [3, 84, 84], 'type': 'rgb'}, 'robot0_eye_in_hand_image': {'shape': [3, 84, 84], 'type': 'rgb'}, 'robot0_eef_pos': {'shape': [3]}, 'robot0_eef_quat': {'shape': [4]}, 'robot0_gripper_qpos': {'shape': [2]}}, 'action': {'shape': [10]}}, 'n_train': 1, 'n_train_vis': 2, 'train_start_idx': 0, 'n_test': 1, 'n_test_vis': 1, 'test_start_seed': 100000, 'max_steps': 400, 'n_obs_steps': 1, 'n_action_steps': 16, 'render_obs_key': 'agentview_image', 'fps': 10, 'crf': 22, 'past_action': False, 'abs_action': True, 'tqdm_interval_sec': 1.0, 'n_envs': 1}, 'dataset': {'_target_': 'diffusion_policy.dataset.robomimic_replay_image_dataset.RobomimicReplayImageLanguageDataset', 'shape_meta': {'obs': {'agentview_image': {'shape': [3, 84, 84], 'type': 'rgb'}, 'robot0_eye_in_hand_image': {'shape': [3, 84, 84], 'type': 'rgb'}, 'robot0_eef_pos': {'shape': [3]}, 'robot0_eef_quat': {'shape': [4]}, 'robot0_gripper_qpos': {'shape': [2]}}, 'action': {'shape': [10]}}, 'dataset_path': '/home/zzy/robotdata/diffusion_policy_data/data/robomimic/datasets/can/ph/image_abs.hdf5', 'horizon': 16, 'pad_before': 0, 'pad_after': 15, 'n_obs_steps': 1, 'abs_action': True, 'rotation_rep': 'rotation_6d', 'use_legacy_normalizer': False, 'use_cache': True, 'seed': 42, 'val_ratio': 0.02, 'load_image_features': True}}}\n"
     ]
    }
   ],
   "source": [
    "path = \"/home/zzy/robot/data/diffusion_policy_data/data/outputs/2024.08.08/15.06.50_train_latent_diffusion_can_image/checkpoints/latest.ckpt\"\n",
    "import torch\n",
    "import dill\n",
    "with open(path, 'rb') as f:\n",
    "    ckpt = torch.load(f, pickle_module=dill)\n",
    "import omegaconf\n",
    "print(ckpt['cfg']['ae_path'])\n",
    "ckpt['cfg']['ae_path'] = \"/home/zzy/robot/data/diffusion_policy_data/data/latent_diffusion_policy_ae/2024-08-07_17-54-46/last.ckpt\"\n",
    "ckpt['cfg']['policy']['ae_path'] = \"/home/zzy/robot/data/diffusion_policy_data/data/latent_diffusion_policy_ae/2024-08-07_17-54-46/last.ckpt\"\n",
    "print(ckpt['cfg'])\n",
    "\n",
    "cfg = omegaconf.DictConfig({'name': 'train_latent_diffusion', '_target_': 'diffusion_policy.workspace.train_latent_diffusion_workspace.TrainLatentDiffusionWorkspace', 'task_name': 'can_image', 'shape_meta': {'obs': {'agentview_image': {'shape': [3, 84, 84], 'type': 'rgb'}, 'robot0_eye_in_hand_image': {'shape': [3, 84, 84], 'type': 'rgb'}, 'robot0_eef_pos': {'shape': [3]}, 'robot0_eef_quat': {'shape': [4]}, 'robot0_gripper_qpos': {'shape': [2]}}, 'action': {'shape': [10]}}, 'exp_name': 'default', 'resume_ckpt_path': None, 'ae_path': '/home/zzy/robot/data/diffusion_policy_data/data/latent_diffusion_policy_ae/2024-08-07_17-54-46/last.ckpt', 'seed': 3407, 'horizon': 16, 'n_obs_steps': 1, 'n_action_steps': 16, 'n_latency_steps': 0, 'dataset_obs_steps': 1, 'past_action_visible': False, 'keypoint_visible_rate': 1.0, 'obs_as_global_cond': True, 'load_image_features': True, 'policy': {'_target_': 'diffusion_policy.policy.latent_diffusion_policy.LatentDiffusionPolicy', 'ae_path': '/home/zzy/robot/data/diffusion_policy_data/data/latent_diffusion_policy_ae/2024-08-07_17-54-46/last.ckpt', 'image_feature_dim': 512, 'load_image_features': True, 'shape_meta': {'obs': {'agentview_image': {'shape': [3, 84, 84], 'type': 'rgb'}, 'robot0_eye_in_hand_image': {'shape': [3, 84, 84], 'type': 'rgb'}, 'robot0_eef_pos': {'shape': [3]}, 'robot0_eef_quat': {'shape': [4]}, 'robot0_gripper_qpos': {'shape': [2]}}, 'action': {'shape': [10]}}, 'num_train_timesteps': 1000, 'beta_start': 0.0001, 'beta_end': 0.02, 'num_inference_timesteps': 1000, 'beta_schedule': 'squaredcos_cap_v2', 'variance_type': 'fixed_small', 'clip_sample': True, 'prediction_type': 'epsilon', 'n_layers': 6, 'n_heads': 8, 'hidden_size': 1024, 'horizon': 16, 'dropout': 0.1, 'lr': 0.0002, 'use_lr_scheduler': True, 'warmup_steps': 1000, 'task_name': 'can_image'}, 'datamodule': {'_target_': 'diffusion_policy.dataset.robomimic_replay_image_dataset.RobomimicImageDatamodule', 'batch_size': 32768, 'num_workers': 4, 'dataset': {'_target_': 'diffusion_policy.dataset.robomimic_replay_image_dataset.RobomimicReplayImageLanguageDataset', 'shape_meta': {'obs': {'agentview_image': {'shape': [3, 84, 84], 'type': 'rgb'}, 'robot0_eye_in_hand_image': {'shape': [3, 84, 84], 'type': 'rgb'}, 'robot0_eef_pos': {'shape': [3]}, 'robot0_eef_quat': {'shape': [4]}, 'robot0_gripper_qpos': {'shape': [2]}}, 'action': {'shape': [10]}}, 'dataset_path': '/home/zzy/robot/data/diffusion_policy_data/data/robomimic/datasets/can/ph/image_abs.hdf5', 'horizon': 16, 'pad_before': 0, 'pad_after': 15, 'n_obs_steps': 1, 'abs_action': True, 'rotation_rep': 'rotation_6d', 'use_legacy_normalizer': False, 'use_cache': True, 'seed': 42, 'val_ratio': 0.02, 'load_image_features': True}}, 'trainer': {'_target_': 'lightning.pytorch.trainer.Trainer', 'max_epochs': 20000, 'max_steps': -1, 'precision': 'bf16-mixed', 'check_val_every_n_epoch': 50, 'log_every_n_steps': 1, 'strategy': 'ddp_find_unused_parameters_true', 'logger': {'_target_': 'lightning.pytorch.loggers.wandb.WandbLogger', 'project': 'latent_diffusion_policy_ldm', 'name': '2024-08-08_15-06-53', 'save_dir': '/home/zzy/robot/data/diffusion_policy_data/data/latent_diffusion_policy_ldm/2024-08-08_15-06-53'}, 'num_sanity_val_steps': 2, 'default_root_dir': '/home/zzy/robot/data/diffusion_policy_data/data/latent_dp_logs/', 'profiler': 'simple', 'gradient_clip_val': 1, 'callbacks': [{'_target_': 'lightning.pytorch.callbacks.ModelCheckpoint', 'save_top_k': 1, 'save_last': True, 'save_weights_only': False, 'monitor': 'val/denoise_loss', 'mode': 'min', 'dirpath': '/home/zzy/robot/data/diffusion_policy_data/data/latent_diffusion_policy_ldm/2024-08-08_15-06-53'}, {'_target_': 'lightning.pytorch.callbacks.TQDMProgressBar'}]}, 'training': {'use_ema': False}, 'logging': {'project': 'diffusion_policy_debug', 'resume': True, 'mode': 'online', 'name': '2024.08.08-15.06.50_train_latent_diffusion_can_image', 'tags': ['train_latent_diffusion', 'can_image', 'default'], 'id': None, 'group': None}, 'checkpoint': {'topk': {'monitor_key': 'test_mean_score', 'mode': 'max', 'k': 5, 'format_str': 'epoch={epoch:04d}-test_mean_score={test_mean_score:.3f}.ckpt'}, 'save_last_ckpt': True, 'save_last_snapshot': False}, 'multi_run': {'run_dir': '/home/zzy/robot/data/diffusion_policy_data/data/data/outputs/2024.08.08/15.06.50_train_latent_diffusion_can_image', 'wandb_name_base': '2024.08.08-15.06.50_train_latent_diffusion_can_image'}, 'task': {'name': 'can_image', 'shape_meta': {'obs': {'agentview_image': {'shape': [3, 84, 84], 'type': 'rgb'}, 'robot0_eye_in_hand_image': {'shape': [3, 84, 84], 'type': 'rgb'}, 'robot0_eef_pos': {'shape': [3]}, 'robot0_eef_quat': {'shape': [4]}, 'robot0_gripper_qpos': {'shape': [2]}}, 'action': {'shape': [10]}}, 'task_name': 'can', 'dataset_type': 'ph', 'dataset_path': '/home/zzy/robot/data/diffusion_policy_data/data/robomimic/datasets/can/ph/image_abs.hdf5', 'abs_action': True, 'env_runner': {'_target_': 'diffusion_policy.env_runner.robomimic_image_runner_ae.RobomimicImageRunnerAE', 'dataset_path': '/home/zzy/robot/data/diffusion_policy_data/data/robomimic/datasets/can/ph/image_abs.hdf5', 'shape_meta': {'obs': {'agentview_image': {'shape': [3, 84, 84], 'type': 'rgb'}, 'robot0_eye_in_hand_image': {'shape': [3, 84, 84], 'type': 'rgb'}, 'robot0_eef_pos': {'shape': [3]}, 'robot0_eef_quat': {'shape': [4]}, 'robot0_gripper_qpos': {'shape': [2]}}, 'action': {'shape': [10]}}, 'n_train': 1, 'n_train_vis': 2, 'train_start_idx': 0, 'n_test': 1, 'n_test_vis': 1, 'test_start_seed': 100000, 'max_steps': 400, 'n_obs_steps': 1, 'n_action_steps': 16, 'render_obs_key': 'agentview_image', 'fps': 10, 'crf': 22, 'past_action': False, 'abs_action': True, 'tqdm_interval_sec': 1.0, 'n_envs': 1}, 'dataset': {'_target_': 'diffusion_policy.dataset.robomimic_replay_image_dataset.RobomimicReplayImageLanguageDataset', 'shape_meta': {'obs': {'agentview_image': {'shape': [3, 84, 84], 'type': 'rgb'}, 'robot0_eye_in_hand_image': {'shape': [3, 84, 84], 'type': 'rgb'}, 'robot0_eef_pos': {'shape': [3]}, 'robot0_eef_quat': {'shape': [4]}, 'robot0_gripper_qpos': {'shape': [2]}}, 'action': {'shape': [10]}}, 'dataset_path': '/home/zzy/robot/data/diffusion_policy_data/data/robomimic/datasets/can/ph/image_abs.hdf5', 'horizon': 16, 'pad_before': 0, 'pad_after': 15, 'n_obs_steps': 1, 'abs_action': True, 'rotation_rep': 'rotation_6d', 'use_legacy_normalizer': False, 'use_cache': True, 'seed': 42, 'val_ratio': 0.02, 'load_image_features': True}}})\n",
    "ckpt['cfg'] = cfg\n",
    "torch.save(ckpt, path, pickle_module=dill)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AnyNode',\n",
       " 'BooleanNode',\n",
       " 'BytesNode',\n",
       " 'Container',\n",
       " 'DictConfig',\n",
       " 'DictKeyType',\n",
       " 'EnumNode',\n",
       " 'FloatNode',\n",
       " 'II',\n",
       " 'IntegerNode',\n",
       " 'KeyValidationError',\n",
       " 'ListConfig',\n",
       " 'MISSING',\n",
       " 'MissingMandatoryValue',\n",
       " 'Node',\n",
       " 'OmegaConf',\n",
       " 'PathNode',\n",
       " 'ReadonlyConfigError',\n",
       " 'Resolver',\n",
       " 'SCMode',\n",
       " 'SI',\n",
       " 'StringNode',\n",
       " 'UnionNode',\n",
       " 'UnsupportedValueType',\n",
       " 'ValidationError',\n",
       " 'ValueNode',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " '_utils',\n",
       " 'base',\n",
       " 'basecontainer',\n",
       " 'dictconfig',\n",
       " 'errors',\n",
       " 'flag_override',\n",
       " 'grammar',\n",
       " 'grammar_parser',\n",
       " 'grammar_visitor',\n",
       " 'listconfig',\n",
       " 'nodes',\n",
       " 'omegaconf',\n",
       " 'open_dict',\n",
       " 'read_write',\n",
       " 'resolvers',\n",
       " 'version']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(omegaconf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robodiff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
